{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Ensemble learning Method\n",
    "This is a basic ensemble learning method programmed with 3 different classification models:**K - Nearest Neighbors, Naive Bayes and a Neural Network**\n",
    "The code below is focused on the following points:\n",
    "- Reading the data from a given file file, transform into a [PCA](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) and train the models\n",
    "- The ensemble's decision boundaries\n",
    "- The ensemble's confussion matrix \n",
    "- The classification results scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impors section\n",
    "This sections imports all the necessary libraries in order for the code to work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports section\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import statistics \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support,log_loss\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Ensemble class\n",
    "This is the definition of the entire Ensemble class. In order to have a better control and atomization of the ensemble methods and process of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self):\n",
    "        self.dataFrame = []\n",
    "        self.nPC = 2\n",
    "        self.nKNN = 2\n",
    "        self.nKFolds = 5\n",
    "        self.hiddenLayers = [10,15,10]\n",
    "        self.activFunction = \"tanh\"\n",
    "        self.nIterations = 2500\n",
    "        self.momentum = 0.9\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.pca = []\n",
    "        self.principalComponents = []\n",
    "        self.finalDf = []\n",
    "        self.text=\"\"\n",
    "        self.nLines=0\n",
    "        self.nAttributes=0\n",
    "        self.nClasses=0\n",
    "        self.attributesName = []\n",
    "        self.data = []\n",
    "        self.dataPerAttribute = []\n",
    "        self.classes = []\n",
    "        self.principalDf = []\n",
    "        self.kn = []\n",
    "        self.nb = []\n",
    "        self.nn = []\n",
    "        self.scores_KK = []\n",
    "        self.scores_KN = []\n",
    "        self.scores_NN = []\n",
    "        self.split_names = []\n",
    "        self.model_names =['K-Nearest Neighbors','Naive Bayes','Neural Network']\n",
    "        self.eclf = []\n",
    "        self.eclf_scores = []\n",
    "    def setDF(self,dataFrame):\n",
    "        self.dataFrame = dataFrame\n",
    "    def readFile(self,filename):\n",
    "        # Read the file in the format given\n",
    "        self.text=\"\"\n",
    "        self.nLines=0\n",
    "        self.nAttributes=0\n",
    "        self.nClasses=0\n",
    "        self.attributesName = []\n",
    "        self.data = []\n",
    "        self.dataPerAttribute = []\n",
    "        self.classes = []\n",
    "        try:\n",
    "            with open(filename,\"r\") as file:\n",
    "                count=0\n",
    "                for line in file:\n",
    "                    if count < 3:\n",
    "                        if count == 0:\n",
    "                            self.nLines = int(line.strip())\n",
    "                        else:\n",
    "                            if count == 1:\n",
    "                                self.nAttributes = int(line.strip())\n",
    "                                for i in range(1,self.nAttributes+1):\n",
    "                                    self.attributesName.append(\"att\"+str(i))\n",
    "                                    self.dataPerAttribute.append([])\n",
    "                            else:\n",
    "                                if count == 2:\n",
    "                                    self.nClasses = int(line.strip())\n",
    "                    else:\n",
    "                        split_string_S = line.strip().split(',')\n",
    "\n",
    "                        count_split = 0\n",
    "                        split_string_n = []\n",
    "                        for split in split_string_S:\n",
    "                            if count_split >= self.nAttributes:\n",
    "                                split_string_n.append(int(split))\n",
    "                            else:\n",
    "                                split_string_n.append(float(split))\n",
    "                            count_split += 1\n",
    "\n",
    "                        self.data.append(split_string_n)\n",
    "                        self.classes.append(split_string_n[self.nAttributes])\n",
    "                        n_attribute = 0\n",
    "                        for attribute in split_string_n:\n",
    "                            if n_attribute >= self.nAttributes:\n",
    "                                break\n",
    "                            else:\n",
    "                                self.dataPerAttribute[n_attribute].append(attribute)\n",
    "                                n_attribute += 1\n",
    "                    count += 1\n",
    "                print(\"EOF reached\")\n",
    "                # Turning the data into a DataFrame python object\n",
    "                columns_ =  self.attributesName[:]\n",
    "                columns_.append(\"target\")\n",
    "                dataFrame = pd.DataFrame(data= self.data, columns=columns_)\n",
    "                self.setDF(dataFrame)\n",
    "        except FileNotFoundError:\n",
    "            text=\"Archivo no existe\"\n",
    "            exit()\n",
    "        finally:\n",
    "            file.close()\n",
    "            # print(str(nLines) + \"\\n\")\n",
    "            # print(str(nAttributes) + \"\\n\")\n",
    "            # print(str(nClasses) + \"\\n\")\n",
    "            # print(attributesName)\n",
    "            # print(data)\n",
    "    def transformData(self,nPC):\n",
    "        self.nPC = nPC\n",
    "        if self.nPC > self.nAttributes:\n",
    "            print(str(self.nAttributes) + \" is the maximum number of principal components\")\n",
    "            self.nPC = min([self.nAttributes,self.nPC])\n",
    "        else:\n",
    "            if self.nPC <= 0:\n",
    "                print(\"1 is the minimum number of principal components\")\n",
    "                self.nPC = max([1,self.nPC])\n",
    "        \n",
    "        # Separating out the features\n",
    "        self.x = self.dataFrame.loc[:, self.attributesName].values\n",
    "        # Separating out the target\n",
    "        self.y = self.dataFrame.loc[:,['target']].values\n",
    "        # Standardizing the features\n",
    "        self.x = StandardScaler().fit_transform(self.x)\n",
    "        \n",
    "        print (\"Creating PCA with \" + str(self.nPC) + \" components\")\n",
    "        self.pca = PCA(n_components=self.nPC)\n",
    "        self.principalComponents = self.pca.fit_transform(self.x)\n",
    "        pc_names = []\n",
    "        for i in range(1,self.nPC+1):\n",
    "            pc_names.append(\"principal component \"+ str(i))\n",
    "\n",
    "        self.principalDf = pd.DataFrame(data = self.principalComponents\n",
    "                     , columns = pc_names)\n",
    "        self.finalDf = pd.concat([self.principalDf, self.dataFrame[['target']]], axis = 1)\n",
    "    def buildModels(self,nKFolds=5,nKNN=5,hiddenLayers=[10,15,10],activFunction='tanh',nIterations=2500,momentum=0.9):\n",
    "        #Building models for the Ensemble\n",
    "        self.nKFolds = nKFolds\n",
    "        self.nKNN = nKNN\n",
    "        self.kn = KNeighborsClassifier(n_neighbors=nKNN)\n",
    "        self.nb = GaussianNB()\n",
    "        self.hiddenLayers = hiddenLayers\n",
    "        self.activFunction = activFunction\n",
    "        self.nIterations = nIterations\n",
    "        self.momentum = momentum\n",
    "        self.nn = MLPClassifier(hidden_layer_sizes=self.hiddenLayers,\n",
    "                                solver='sgd',\n",
    "                                activation=self.activFunction,\n",
    "                                max_iter=self.nIterations,\n",
    "                                momentum=self.momentum)\n",
    "        \n",
    "    def getScoreModels(self):\n",
    "        self.scores_KN = cross_val_score(self.kn, self.principalComponents, self.y.ravel(), cv=self.nKFolds)\n",
    "        self.scores_NB = cross_val_score(self.nb, self.principalComponents, self.y.ravel(), cv=self.nKFolds)\n",
    "        self.scores_NN = cross_val_score(self.nn, self.principalComponents, self.y.ravel(), cv=self.nKFolds)\n",
    "        self.split_names= []\n",
    "        \n",
    "        for i in range(1,self.nKFolds+1):\n",
    "            self.split_names.append(\"Fold \"+str(i))\n",
    "            \n",
    "        kk_df = pd.DataFrame(self.scores_KN,\n",
    "                           columns=['K-Nearest Neighbors'],\n",
    "                             index=self.split_names)\n",
    "        nb_df = pd.DataFrame(self.scores_NB,\n",
    "                           columns=['Naive Bayes'],\n",
    "                             index=self.split_names)\n",
    "        nn_df = pd.DataFrame(self.scores_NN,\n",
    "                           columns=['Neural Network'],\n",
    "                             index=self.split_names)\n",
    "        avg_models = [np.mean(self.scores_KN),\n",
    "                     np.mean(self.scores_NB),\n",
    "                     np.mean(self.scores_NN)]\n",
    "        avg_df = pd.DataFrame(avg_models,\n",
    "                              columns=['Normal Cross Validation Average'],index=self.model_names)\n",
    "        return kk_df,nb_df,nn_df,avg_df\n",
    "    def buildEnsemble(self,vote='hard'):\n",
    "        self.eclf = VotingClassifier(estimators=[('kn', self.kn), \n",
    "                                                 ('nb', self.nb), \n",
    "                                                 ('nnet', self.nn)], \n",
    "                                     voting=vote)\n",
    "    def trainEnsemble(self):\n",
    "        self.eclf_scores = cross_val_score(self.eclf, \n",
    "                                           self.principalComponents, \n",
    "                                           self.y.ravel(), \n",
    "                                           scoring='accuracy', \n",
    "                                           cv=self.nKFolds)\n",
    "        eclf_df = pd.DataFrame(self.eclf_scores,\n",
    "                           columns=['Ensemble'],\n",
    "                             index=self.split_names)\n",
    "        #print(\"Accuracy: %0.4f (+/- %0.4f)\" % (eclf_scores.mean(), eclf_scores.std() * 2))\n",
    "        return eclf_df,self.eclf_scores.mean(),self.eclf_scores.std()\n",
    "    def plot2DimPCA(self):\n",
    "        fig = plt.figure(figsize = (8,8))\n",
    "        ax = fig.add_subplot(1,1,1) \n",
    "        ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "        ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "        ax.set_title('2 component PCA', fontsize = 20)\n",
    "        targets = range(0,self.nClasses)\n",
    "        targetsLabels = []\n",
    "        colors_ = ['r', 'g', 'b','c','m','y','k','w']\n",
    "        colors = []\n",
    "        for i in targets:\n",
    "            targetsLabels.append(\"Class \"+str(i))\n",
    "            colors.append(colors_[i])\n",
    "        for target, color in zip(targets,colors):\n",
    "            indicesToKeep = self.finalDf['target'] == target\n",
    "            ax.scatter(self.finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "                       , self.finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "                       , c = color\n",
    "                       , s = 50)\n",
    "        ax.legend(targetsLabels)\n",
    "        ax.grid()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    def plotDecisionBounderies(self):\n",
    "        gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "        labels = ['K Nearest Neighbors', 'Naive Bayes', 'Neural Network', 'Ensemble']\n",
    "        for clf, lab, grd in zip([self.kn, self.nb, self.nn, self.eclf],\n",
    "                                 labels,\n",
    "                                 itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "            x = self.finalDf.loc[:,['principal component 1','principal component 2']].values\n",
    "            clf.fit(x, self.y.ravel())\n",
    "            ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "            fig = plot_decision_regions(X=x, y=self.y.ravel(), clf=clf)\n",
    "            plt.title(lab)\n",
    "    def printVarianceRatio(self):\n",
    "        # Variance explained\n",
    "        print(\"-------Variance per principal component-------\")\n",
    "        print (self.pca.explained_variance_)\n",
    "        print(\"-------Variance ratio per principal component-------\")\n",
    "        print (self.pca.explained_variance_ratio_)\n",
    "        print(\"-------Variance ratio accumulated sum per principal component-------\")\n",
    "        print (self.pca.explained_variance_ratio_.cumsum())\n",
    "basic_ensemble = Ensemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the configuration for the ensemble and intern classification models\n",
    "Here the user must type the following fields to begin with the ensemble's training and testing:\n",
    "- The initial data file with the specified format\n",
    "- The number of principal components for the PCA\n",
    "- The number of K-Folds for Cross Validation\n",
    "- The value of K-Nearest Neighbors to consider\n",
    "- The number of Hidden Layers for the Neural Network and the number of perceptrons for each layer\n",
    "- The activation function for the Newural Network: {identity, logistic, tanh, relu}\n",
    "- The number of iterations for the Neural Network\n",
    "- The value of momentum for the Neural Network, a floating value between 0 and 1 inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOF reached\n",
      "Number of principal components:30\n",
      "Creating PCA with 30 components\n",
      "Enter number of K-Folds for Cross Validation: 5\n",
      "Enter value of K-Nearest Neighbors: 5\n",
      "Enter number of Hidden Layers for the Neural Network: 3\n",
      "Number of perceptrons for the 1-th layer: 10\n",
      "Number of perceptrons for the 2-th layer: 15\n",
      "Number of perceptrons for the 3-th layer: 10\n",
      "activation{identity, logistic, tanh, relu}: tanh\n",
      "Number of iterations for the Neural Network: 3000\n",
      "Momentum for the Neural Network[0-1]: 0.8\n"
     ]
    }
   ],
   "source": [
    "basic_ensemble.readFile(\"cre300.txt\")\n",
    "basic_ensemble.transformData(int(input('Number of principal components:')))\n",
    "\n",
    "folds = int(input(\"Enter number of K-Folds for Cross Validation: \"))\n",
    "neighbors = int(input(\"Enter value of K-Nearest Neighbors: \"))\n",
    "hLayers = int(input(\"Enter number of Hidden Layers for the Neural Network: \"))\n",
    "layersSize=[]\n",
    "for i in range(1,hLayers+1):\n",
    "    layersSize.append(int(input(\"Number of perceptrons for the \"+str(i)+\"-th layer: \")))\n",
    "activationFunction=input(\"activation{identity, logistic, tanh, relu}: \")\n",
    "epochs = int(input(\"Number of iterations for the Neural Network: \"))\n",
    "moment = float(input(\"Momentum for the Neural Network[0-1]: \"))\n",
    "\n",
    "basic_ensemble.buildModels(nKFolds=folds,\n",
    "                           nKNN=neighbors,\n",
    "                           hiddenLayers=layersSize,\n",
    "                           activFunction=activationFunction,\n",
    "                           nIterations=epochs,\n",
    "                           momentum=moment)\n",
    "\n",
    "d1,d2,d3,a = basic_ensemble.getScoreModels()\n",
    "basic_ensemble.buildEnsemble()\n",
    "e_df,e_mean,e_std = basic_ensemble.trainEnsemble()\n",
    "e_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the 2-dimension PCA data\n",
    "Now we will give a visual representation of our dataset using the Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the 2-dimension PCA\n",
    "basic_ensemble.plot2DimPCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance explained\n",
    "We print the variance per principal component in order to give a metric of how well represented our data is. \n",
    "The first array shows the variance int the data per principal component, the second one, shows for each component, the variance ratio of the entire data set, that it represents, and the third and last one, shows the accumulated sum of the variance ratio in each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_ensemble.printVarianceRatio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Reports\n",
    "The following section will plot the classification results for every single fold in the cross validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Classification Reports\n",
    "classes = []\n",
    "for i in range(0,basic_ensemble.nClasses):\n",
    "    classes.append(str(i))\n",
    "i = 0\n",
    "KFolds = StratifiedKFold(n_splits=basic_ensemble.nKFolds)\n",
    "for train_index,test_index in KFolds.split(X=basic_ensemble.principalComponents,y=basic_ensemble.y):\n",
    "    X_train,X_test,y_train,y_test = basic_ensemble.principalComponents[train_index], basic_ensemble.principalComponents[test_index], basic_ensemble.y[train_index], basic_ensemble.y[test_index]\n",
    "    visualizer = ClassificationReport(basic_ensemble.eclf, classes=classes, support=True)\n",
    "    visualizer.fit(X_train, y_train.ravel())        # Fit the visualizer and the model\n",
    "    visualizer.score(X_test, y_test.ravel())        # Evaluate the model on the test data\n",
    "    print(\"-----------------Fold \"+str(i)+\"--------------------\")\n",
    "    visualizer.show() \n",
    "    i += 1\n",
    "    print(\"----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundaries\n",
    "Now we will plot the decision boundaries for every intern classification model in the ensemble learning method and for the ensemble as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ensemble and model's decision boundaries\n",
    "basic_ensemble.plotDecisionBounderies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confussion matrix\n",
    "In order to see how well our ensemble learning method is classifying, here is plotted the ensemble's confussion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confussion matrix\n",
    "y_pred = cross_val_predict(basic_ensemble.eclf, \n",
    "                           basic_ensemble.principalComponents, \n",
    "                           basic_ensemble.y.ravel(), \n",
    "                           cv=basic_ensemble.nKFolds)\n",
    "\n",
    "# Creates a confusion matrix\n",
    "cm = confusion_matrix(basic_ensemble.y.ravel(), y_pred) \n",
    "\n",
    "# Transform to df for easier plotting\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = classes, \n",
    "                     columns = classes)\n",
    "\n",
    "#plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Ensemble classification \\nAccuracy:{0:.3f}'.format(accuracy_score(basic_ensemble.y.ravel(), y_pred)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot of the classification results\n",
    "Finally we have the classification results scatter plot to get a visual representation of the ensemble's classification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = []\n",
    "y_array = []\n",
    "diff = 0.3\n",
    "for i in range(0,cm.shape[0]):\n",
    "    for j in range (0,cm.shape[1]):\n",
    "        for c in range(0,cm[i][j]): \n",
    "            i_ = float(i)\n",
    "            j_ = float(j)\n",
    "            x_ = random.uniform(i_ - diff, i_ + diff)\n",
    "            y_ = random.uniform(j_ - diff, j_ + diff)\n",
    "            x_array.append(x_)\n",
    "            y_array.append(y_)\n",
    "rng = np.random.RandomState(0)\n",
    "colors = rng.rand(basic_ensemble.nLines)\n",
    "plt.scatter(x_array, y_array,c=colors, alpha=0.3,\n",
    "            cmap='viridis')\n",
    "plt.xlabel(\"True label\")\n",
    "plt.ylabel(\"Predicted Label\");\n",
    "plt.colorbar();  # show color scale\n",
    "plt.title(\"Scatterplot of the classification results\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
